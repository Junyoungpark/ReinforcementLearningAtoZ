{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..') # add project root to the python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "\n",
    "from src.part3.MLP import MultiLayerPerceptron as MLP\n",
    "from src.part4.PolicyGradient import REINFORCE\n",
    "from src.common.train_utils import EMAMeter, to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joon0\\Anaconda3\\envs\\gpu_torch130\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE 알고리즘으로, 'CartPole-V1'을 정복하기.\n",
    "\n",
    "이번 실습에서는 `REINFORCE` 알고리즘으로 `CartPole-V1` MDP를 정복해볼까요?\n",
    "`REINFORCE`의 의사코드는 다음과 같습니다.\n",
    "\n",
    "<img src=\"./images/REINFORCE.png\" width=\"60%\" height=\"40%\" title=\"REINFORCE\" alt=\"REINFORCE\"></img>\n",
    "\n",
    "`REINFORCE` 알고리즘을 파이썬으로는 어떻게 구현할까요?\n",
    "```python\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "class REINFORCE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 policy: nn.Module,\n",
    "                 gamma: float = 1.0,\n",
    "                 lr: float = 0.0002):\n",
    "        super(REINFORCE, self).__init__()\n",
    "        self.policy = policy  # make sure that 'policy' returns logits!\n",
    "        self.gamma = gamma\n",
    "        self.opt = torch.optim.Adam(params=self.policy.parameters(),\n",
    "                                    lr=lr)\n",
    "\n",
    "        self._eps = 1e-25\n",
    "\n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            logits = self.policy(state)\n",
    "            dist = Categorical(logits=logits)\n",
    "            a = dist.sample()  # sample action from softmax policy\n",
    "        return a\n",
    "```\n",
    "\n",
    "### Logit? Catergorical?\n",
    "\n",
    "Catergorical distribution (다항분포) 는 이항분포의 확장판입니다. <br>\n",
    "\n",
    ">이항분포는 사건의 종류가 2개일 상황을 (예를 들어 한번 동전을 던져서 앞/뒤가 나올 확률) 모델링 할 때 자주 쓰이게 됩니다.\n",
    "이항 분포의 매개변수는 $p$ 로 하나의 사건이 나올 확률을 표현합니다. 이때 $0 \\leq p \\leq 1$ 를 따르게 됩니다.\n",
    "\n",
    "그렇다면 사건의 종류가 2개보다 많은 경우는 어떻게 할 수 있을까요? 그런 경우에 다항 분포를 사용해볼 수 있습니다.\n",
    "\n",
    "> 사건의 종류가 $n$ 개 일때, 다항분포의 매개변수들 $(p_1, ..., p_n)$ 이 되며, $p_i$ 는 각 사건이 일어날 확률을 표현합니다.\n",
    "> 따라서 $p_i$ 는 모두 0보다 크거나 같고, $\\sum_{i=1}^{n} p_i =1$ 을 만족시켜야 합니다.\n",
    "\n",
    "자 그러면, 이제 신경망 `self.policy(state)` 출력이 위의 조건을 만족시키게 만들 수 있을까요? 다양한 방법이 존재하지만, 가장 일반적인 선택은\n",
    "softmax라는 연산자를 활용하는 것입니다.\n",
    "\n",
    "$$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{n}e^{z_j}} \\forall i=1,...,n$$\n",
    "\n",
    "임의의 값들의 집합 $\\{{z_i}\\}_{i=1}^{n}$ 에 위에서 정의한 softmax 연산자를 가해주면, 모든 값들은 0 이상의 값으로 바뀌고 값들의 합은 1.0 이 되게 됩니다.\n",
    "이때, $z_i$ 를 일반적으로 logit 이라고 부릅니다.\n",
    "\n",
    "### Implemenation tip! \n",
    "> 일반적으로 NN계산 중에 `exp()` 연산은 선호되지 않습니다. 입력값이 작거나/클때 쉽게 값이 underflow/overflow 할 염려가 있기 때문인데요.\n",
    "> 신경써서 구현하면 그런 문제들을 완화할 수 있지만, 매번 신경쓰기가 번거롭습니다. <br>\n",
    "\n",
    "> softmax + Catergorical 사용같이 자주 사용되는 패턴은 pytorch 수준에서 최대한 numerical stability를 가지도록 만들어져 있습니다. 그 예시 중에 하나로 pytorch 의 `Catergorical`은 logit값으로 부터 생성하는것이 가능합니다. 따라서, 직접 softmax를 계산한 후 `Catergorical` 매개변수로 사용하는것 보다는 `Catergorical`분포의 매개변수로 `logit`을 넘겨주는 것이 조금 더 안전합니다.\n",
    "\n",
    "\n",
    "```python\n",
    "    @staticmethod\n",
    "    def _pre_process_inputs(episode):\n",
    "        states, actions, rewards = episode\n",
    "\n",
    "        # assume inputs as follows\n",
    "        # s : torch.tensor [num.steps x state_dim]\n",
    "        # a : torch.tensor [num.steps]\n",
    "        # r : torch.tensor [num.steps]\n",
    "\n",
    "        # reversing inputs\n",
    "        states = states.flip(dims=[0])\n",
    "        actions = actions.flip(dims=[0])\n",
    "        rewards = rewards.flip(dims=[0])\n",
    "        return states, actions, rewards\n",
    "\n",
    "    def update(self, episode):\n",
    "        # sample-by-sample update version of REINFORCE\n",
    "        # sample-by-sample update version is highly inefficient in computation\n",
    "        states, actions, rewards = self._pre_process_inputs(episode)\n",
    "\n",
    "        g = 0\n",
    "        for s, a, r in zip(states, actions, rewards):\n",
    "            g = r + self.gamma * g\n",
    "            dist = Categorical(logits=self.policy(s))\n",
    "            prob = dist.probs[a]\n",
    "\n",
    "            # Don't forget to put '-' in the front of pg_loss !!!!!!!!!!!!!!!!\n",
    "            # the default behavior of pytorch's optimizer is to minimize the targets\n",
    "            # add 'self_eps' to prevent numerical problems of logarithms\n",
    "            pg_loss = - torch.log(prob + self._eps) * g\n",
    "\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "            pg_loss.backward()\n",
    "            self.opt.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(s_dim, a_dim, [128])\n",
    "agent = REINFORCE(net)\n",
    "ema = EMAMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 || EMA: 15.0 \n",
      "Episode 500 || EMA: 28.274833570317945 \n",
      "Episode 1000 || EMA: 55.207220467799196 \n",
      "Episode 1500 || EMA: 60.38703727608534 \n",
      "Episode 2000 || EMA: 115.56517011414884 \n",
      "Episode 2500 || EMA: 185.65287882975048 \n",
      "Episode 3000 || EMA: 161.19202441967337 \n",
      "Episode 3500 || EMA: 254.1207580027799 \n",
      "Episode 4000 || EMA: 341.3443486106662 \n",
      "Episode 4500 || EMA: 445.50876492812426 \n",
      "Episode 5000 || EMA: 316.0545563651241 \n",
      "Episode 5500 || EMA: 497.89040434755145 \n",
      "Episode 6000 || EMA: 427.0517173284881 \n",
      "Episode 6500 || EMA: 468.69471805294796 \n",
      "Episode 7000 || EMA: 499.99999999637424 \n",
      "Episode 7500 || EMA: 499.01531979523133 \n",
      "Episode 8000 || EMA: 283.0781249999658 \n",
      "Episode 8500 || EMA: 499.9999978169799 \n",
      "Episode 9000 || EMA: 499.96826171875 \n",
      "Episode 9500 || EMA: 499.99826049804676 \n"
     ]
    }
   ],
   "source": [
    "n_eps = 10000\n",
    "print_every = 500\n",
    "\n",
    "for ep in range(n_eps):\n",
    "    s = env.reset()\n",
    "    cum_r = 0\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    while True:\n",
    "        s = to_tensor(s, size=(1, 4))\n",
    "        a = agent.get_action(s)\n",
    "        ns, r, done, info = env.step(a.item())\n",
    "\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = ns\n",
    "        cum_r += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    ema.update(cum_r)\n",
    "    if ep % print_every == 0:\n",
    "        print(\"Episode {} || EMA: {} \".format(ep, ema.s))\n",
    "\n",
    "    states = torch.cat(states, dim=0)  # torch.tensor [num. steps x state dim]\n",
    "    actions = torch.stack(actions).squeeze()  # torch.tensor [num. steps]\n",
    "    rewards = torch.tensor(rewards)  # torch.tensor [num. steps]\n",
    "\n",
    "    episode = (states, actions, rewards)\n",
    "    agent.update_episode(episode, use_norm=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
