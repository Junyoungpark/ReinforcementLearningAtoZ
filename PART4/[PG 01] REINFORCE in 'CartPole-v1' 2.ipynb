{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..') # add project root to the python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "from src.part3.MLP import MultiLayerPerceptron as MLP\n",
    "from src.part4.PolicyGradient import REINFORCE\n",
    "from src.common.train_utils import EMAMeter, to_tensor\n",
    "from src.common.memory.episodic_memory import EpisodicMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joon0\\Anaconda3\\envs\\gpu_torch130\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch REINFORCE 알고리즘 실습\n",
    "\n",
    "이전 실습에서 배웠던 REINFORCE 알고리즘은 새로운 Episode가 주어지면 매 step에 대해 업데이트를 수행했었던 것 기억하시죠? 하지만 많은 경우, REINFORCE 알고리즘 및 PG 알고리즘을 구현할 때는 계산상의 효율성을 위해서, Episode (혹은 여러개의 Episode들) 단위로 업데이트하는 경우가 좀더 흔합니다. 이번에는 Episode들을 배칭해서 업데이트하는, Batch update 버젼의 REINFORCE 알고리즘들을 구현해봅시다. \n",
    "\n",
    "### 수업에서 배운 수식을 잠깐 복습해볼까요?\n",
    "\n",
    ">REINFORCE (1992):\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}\\ln \\pi_{\\theta}(A_t|S_t)G_t$$\n",
    "\n",
    ">Episodic update REINFORCE\n",
    "$$\\theta \\leftarrow \\theta + \\alpha\\frac{1}{T}\\biggr(\\sum_{t=1}^{T}\\nabla_{\\theta}\\ln \\pi_{\\theta}(A_t|S_t)G_t\\biggr)$$\n",
    "\n",
    ">Batch episodic update REINFORCE:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha\\frac{1}{\\sum_{i=1}^{N} T^{i}} \\biggr(\\sum_{i=1}^{N}\\sum_{t=1}^{T^i}\\nabla_{\\theta}\\ln \\pi_{\\theta}(A_t^i|S_t^i)G_t^i\\biggr)$$\n",
    "\n",
    "$i$ 는 에피소드 인덱스, $T^i$는 에피소드 $i$ 의 길이, $A_t^i, S_t^i, G_t^i$는 에피소드 $i$ 의 $t$ 시점의 행동, 상태, 리턴을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(s_dim, a_dim, [128])\n",
    "agent = REINFORCE(net)\n",
    "memory = EpisodicMemory(max_size=100, gamma=1.0)\n",
    "ema = EMAMeter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New kids on the block `EpisodicMemory`\n",
    "\n",
    "`EpisodicMemory` 라는 이전까지 보지 못한 새로운 녀석이 나타났네요. `EpisodicMemory`는 sample $(s_t, a_t, r_t, s_{t+1}, \\text{done})$ 을 저장해주고 리턴을 계산해주는 장치입니다. 한번 살펴보도록 할까요?\n",
    "\n",
    "> 전체 코드는 `src.common.memory.episodic_memory.py` 를 참조하세요. <br>\n",
    "> 참고) `EpisodicMemory`는 PG 구현에 필수적인것은 아닙니다. 하지만 확실히 구현할때 신경쓸 부분이 적어져서 좋습니다.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from collections import deque\n",
    "from src.common.memory.trajectory import Trajectory\n",
    "\n",
    "\n",
    "class EpisodicMemory:\n",
    "\n",
    "    def __init__(self, max_size: int, gamma: float):\n",
    "        self.max_size = max_size  # maximum number of trajectories\n",
    "        self.gamma = gamma\n",
    "        self.trajectories = deque(maxlen=max_size)\n",
    "        self._trajectory = Trajectory(gamma=gamma)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self._trajectory.push(state, action, reward, next_state, done)\n",
    "        if done:\n",
    "            self.trajectories.append(self._trajectory)\n",
    "            self._trajectory = Trajectory(gamma=self.gamma)\n",
    "\n",
    "    def reset(self):\n",
    "        self.trajectories.clear()\n",
    "        self._trajectory = Trajectory(gamma=self.gamma)\n",
    "\n",
    "    def get_samples(self):\n",
    "        # May require some modification depending on the environment.\n",
    "        \n",
    "        states, actions, rewards, next_states, dones, returns = [], [], [], [], [], []\n",
    "        while self.trajectories:\n",
    "            traj = self.trajectories.pop()\n",
    "            s, a, r, ns, done, g = traj.get_samples()\n",
    "            states.append(torch.cat(s, dim=0))\n",
    "            actions.append(torch.cat(a, dim=0))\n",
    "            rewards.append(torch.cat(r, dim=0))\n",
    "            next_states.append(torch.cat(ns, dim=0))\n",
    "            dones.append(torch.cat(done, dim=0))\n",
    "            returns.append(torch.cat(g, dim=0))\n",
    "\n",
    "        states = torch.cat(states, dim=0)\n",
    "        actions = torch.cat(actions, dim=0)\n",
    "        rewards = torch.cat(rewards, dim=0)\n",
    "        next_states = torch.cat(next_states, dim=0)\n",
    "        dones = torch.cat(dones, dim=0)\n",
    "        returns = torch.cat(returns, dim=0)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, returns\n",
    "    \n",
    "```\n",
    "`Trajectory` 전체 코드는 `src.common.memory.trajectory.py`를 참조해주세요.\n",
    "```python\n",
    "class Trajectory:\n",
    "\n",
    "    def __init__(self, gamma: float):\n",
    "        self.gamma = gamma\n",
    "        self.states = list()\n",
    "        self.actions = list()\n",
    "        self.rewards = list()\n",
    "        self.next_states = list()\n",
    "        self.dones = list()\n",
    "\n",
    "        self.length = 0\n",
    "        self.returns = None\n",
    "        self._discounted = False\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if done and self._discounted:\n",
    "            raise RuntimeError(\"done is given at least two times!\")\n",
    "\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.dones.append(done)\n",
    "        self.length += 1\n",
    "\n",
    "        if done and not self._discounted:\n",
    "            # compute returns\n",
    "            self.compute_return()\n",
    "\n",
    "    def compute_return(self):\n",
    "        rewards = self.rewards\n",
    "        returns = list()\n",
    "\n",
    "        g = 0\n",
    "        # iterating returns in reverse order\n",
    "        for r in rewards[::-1]:\n",
    "            g = r + self.gamma * g\n",
    "            returns.insert(0, g)\n",
    "        self.returns = returns\n",
    "        self._discounted = True\n",
    "\n",
    "    def get_samples(self):\n",
    "        return self.states, self.actions, self.rewards, self.next_states, self.dones, self.returns\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 || EMA: tensor([[20.]]) \n"
     ]
    }
   ],
   "source": [
    "n_eps = 1000\n",
    "update_every = 2 # Update every `update_every` episodes\n",
    "print_every = 50\n",
    "\n",
    "for ep in range(n_eps):\n",
    "    s = env.reset()\n",
    "    cum_r = 0\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    while True:\n",
    "        s = to_tensor(s, size=(1, 4))\n",
    "        a = agent.get_action(s)\n",
    "        ns, r, done, info = env.step(a.item())\n",
    "        \n",
    "        # preprocess data\n",
    "        r = torch.ones(1,1) * r\n",
    "        done = torch.ones(1,1) * done\n",
    "        \n",
    "        memory.push(s,a,r,torch.tensor(ns),done)\n",
    "                \n",
    "        s = ns\n",
    "        cum_r += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    ema.update(cum_r)\n",
    "    if ep % print_every == 0:\n",
    "        print(\"Episode {} || EMA: {} \".format(ep, ema.s))\n",
    "    \n",
    "    if ep % update_every == 0:\n",
    "        s,a, _, _, done, g = memory.get_samples()\n",
    "        agent.update_episodes(s, a, g, use_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `wandb` 를 이용한 Happy logging 및 저장\n",
    "\n",
    "> 저는 `wandb`와 아무런 이해관계도 없습니다. 제가 여러가지를 써보니까 이게 가장 편하고 강력하더라고요.\n",
    "\n",
    "REINFORCE 및 다른 강화학습 알고리즘을 할 때, 수동으로 결과를 저장/출력 (logging)하고 여러번의 반복실험을 거쳐서 결과를 저장했던 지난날들이 있었죠? \n",
    "\n",
    "1. 이런 방식은 새로운 모델을 만들고 훈련할때마다 로깅하는 방식이 달라질수 있기 때문에, 매번 새롭게 로깅하는 코드를 작성해야 했고\n",
    "2. 모델을 저장할 때도 저장된 모델이, 매번 어떤 hyperparameter 였는지 혹은 어떤 알고리즘이었는지 추가적으로 따로 관리해야하는 단점이 있습니다.\n",
    "\n",
    "사소해 보이는 단점이긴 하지만, 연구나 대규모의 실험을 진행하다보면 이 과정이 매우 번거롭습니다. 이번 기회에 `wandb` (Weight AND Bias)로 앞서 이야기한 과정을 자동화하고, 추가적으로 모델의 학습과정 및 저장된 모델을 웹 기반으로 관리할 수 있는 방법에 대해서 이야기 해보면 좋겠네요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb 설정하기\n",
    "\n",
    "한번 `wandb`를 설정해보도록 할까요?\n",
    "\n",
    "#### 1. wandb 설치하기\n",
    "```bash\n",
    "pip install wandb\n",
    "```\n",
    "\n",
    "#### 2. wandb 계정 만들기 <br>\n",
    "wandb [홈페이지](https://www.wandb.com/) 에 접속해서 계정을 만들어주세요. 학교에 재학하는 중이시라면 학교 계정 (ex. nonexist@kaist.ac.kr) 으로 계정을 만들면 wandb의 pro 기능을 무료로 사용할 수 있으니 학교 계정으로 가입하는것도 좋은 선택입니다.\n",
    "\n",
    "#### 3. 로컬에서 wandb login 하기\n",
    "```bash\n",
    "wandb login 'your-api-key'\n",
    "```\n",
    "\n",
    "`'your-api-key'`는 https://app.wandb.ai/settings 페이지에서 `API keys` 라는 항목에서 찾을 수 있습니다.\n",
    "\n",
    "더 많은 wandb 설명 문서는 [여기](https://docs.wandb.com/)를 참조해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb로 logging 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import json\n",
    "import os; os.environ['WANDB_NOTEBOOK_NAME'] = 'jyp'\n",
    "\n",
    "net = MLP(s_dim, a_dim, [128])\n",
    "agent = REINFORCE(net)\n",
    "memory = EpisodicMemory(max_size=100, gamma=1.0)\n",
    "\n",
    "n_eps = 1000\n",
    "update_every = 2 # Update every `update_every` episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wandb.config`로 모델의 Configuration 을 기록해보기\n",
    "\n",
    "일단 하나의 실험의 configuration을 기록해보도록 할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/junyoung-park/my-first-wandb-project\" target=\"_blank\">https://app.wandb.ai/junyoung-park/my-first-wandb-project</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/junyoung-park/my-first-wandb-project/runs/gbnuj5vm\" target=\"_blank\">https://app.wandb.ai/junyoung-park/my-first-wandb-project/runs/gbnuj5vm</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/junyoung-park/my-first-wandb-project/runs/gbnuj5vm"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict()\n",
    "config['n_eps'] = n_eps\n",
    "config['update_every'] = update_every\n",
    "\n",
    "wandb.init(project='my-first-wandb-project',\n",
    "           config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wandb.log()`로 logging 하기\n",
    "\n",
    "`wandb`의 정말 큰 장점중 하나는 원래 code에 단 몇줄의 수정으로 원하는 값들을 logging 할수 있다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joon0\\Anaconda3\\envs\\gpu_torch130\\lib\\site-packages\\wandb\\compat\\tempfile.py:64: UserWarning: Couldn't remove temp directory C:\\Users\\joon0\\AppData\\Local\\Temp\\tmpol6fbhyhwandb\n",
      "  _warnings.warn(\"Couldn't remove temp directory %s\" % name)\n"
     ]
    }
   ],
   "source": [
    "for ep in range(n_eps):\n",
    "    s = env.reset()\n",
    "    cum_r = 0\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    while True:\n",
    "        s = to_tensor(s, size=(1, 4))\n",
    "        a = agent.get_action(s)\n",
    "        ns, r, done, info = env.step(a.item())\n",
    "        \n",
    "        # preprocess data\n",
    "        r = torch.ones(1,1) * r\n",
    "        done = torch.ones(1,1) * done\n",
    "        \n",
    "        memory.push(s,a,r,torch.tensor(ns),done)\n",
    "                \n",
    "        s = ns\n",
    "        cum_r += r\n",
    "        if done:\n",
    "            break    \n",
    "    \n",
    "    if ep % update_every == 0:\n",
    "        s,a, _, _, done, g = memory.get_samples()\n",
    "        agent.update_episodes(s, a, g, use_norm=True)\n",
    "    \n",
    "    log_dict = dict()\n",
    "    log_dict['cum_return'] = cum_r\n",
    "    wandb.log(log_dict)\n",
    "\n",
    "\n",
    "# Save model and experiment configuration\n",
    "json_val = json.dumps(config)\n",
    "with open(join(wandb.run.dir, 'config.json'), 'w') as f:\n",
    "    json.dump(json_val, f)\n",
    "    \n",
    "torch.save(agent.state_dict(), join(wandb.run.dir, 'model.pt'))\n",
    "\n",
    "# close wandb session\n",
    "wandb.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원격 저장소에서 저장된 파일 불러오기\n",
    "\n",
    "`wandb`를 활용하면 손쉽게 logging을 할 수 있는것 외에도 장점이 많습니다. `wandb`의 장점은 Hyperparameter를 최적화해주는 hyperparmeter sweeping, 수려한 plotting 등이 있지만, 그 외에도 이번 실습에서 확인해볼 기능은 원격저장소에서 저장된 파일을 내려받는 기능입니다.\n",
    "\n",
    "이 기능을 활용해서, 특정 `wandb` run 에 해당하는 모델의 파라미터를 불러와볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run_path = 'junyoung-park/my-first-wandb-project/gbnuj5vm'\n",
    "model_config_path = wandb.restore('config.json', wandb_run_path, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_config_path.name, \"r\") as f:\n",
    "    config_str = json.load(f)\n",
    "    config_loaded = json.loads(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config\n",
      "{'n_eps': 1000, 'update_every': 2}\n",
      "loaded config\n",
      "{'n_eps': 1000, 'update_every': 2}\n"
     ]
    }
   ],
   "source": [
    "print(\"Config\")\n",
    "print(config)\n",
    "\n",
    "print(\"loaded config\")\n",
    "print(config_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = wandb.restore('model.pt', wandb_run_path, replace=True)\n",
    "\n",
    "net2 = MLP(s_dim, a_dim, [128])\n",
    "agent2 = REINFORCE(net2)\n",
    "\n",
    "agent2.load_state_dict(torch.load(model_path.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('policy.layers.0.weight',\n",
       "              tensor([[ 4.3611e-01, -2.0396e-01,  3.9874e-02,  1.3420e-01],\n",
       "                      [ 2.8729e-02,  2.1325e-01, -5.0081e-03,  2.3193e-01],\n",
       "                      [ 4.4421e-01, -1.3185e-01, -4.3520e-01, -9.0893e-02],\n",
       "                      [ 4.3958e-01, -4.1482e-01, -2.6271e-01,  3.3908e-01],\n",
       "                      [-2.9901e-01,  2.2829e-01,  9.9549e-02,  2.8231e-01],\n",
       "                      [ 3.2525e-01, -6.7828e-02,  1.6278e-01, -4.3135e-01],\n",
       "                      [-3.5815e-01,  3.9466e-02, -2.6664e-01,  2.2111e-01],\n",
       "                      [ 4.8813e-02,  1.6742e-02,  3.5585e-02, -3.1229e-01],\n",
       "                      [-4.8922e-01, -5.0620e-01,  3.1691e-02,  2.2765e-01],\n",
       "                      [ 1.6877e-01, -4.4479e-01, -1.5556e-01, -4.8567e-02],\n",
       "                      [ 2.4891e-01,  3.2433e-01, -3.6835e-01, -1.7808e-02],\n",
       "                      [-1.5222e-01,  1.0982e-01, -2.9689e-01,  3.0166e-01],\n",
       "                      [-4.2553e-01,  3.8729e-01,  4.7464e-01,  2.3338e-01],\n",
       "                      [ 1.9807e-01, -1.8616e-01, -5.3848e-01, -1.9335e-01],\n",
       "                      [ 1.3777e-01, -2.9924e-01,  2.7423e-01, -1.0082e-01],\n",
       "                      [ 2.9241e-01, -3.2148e-01,  4.8942e-01, -1.3820e-01],\n",
       "                      [-8.2829e-02, -7.2250e-02,  1.0225e-01, -1.0163e-01],\n",
       "                      [ 4.1562e-01,  2.2538e-01, -1.3217e-01,  3.9419e-01],\n",
       "                      [-4.4348e-01, -1.6705e-01,  7.2327e-02, -6.5985e-02],\n",
       "                      [-2.8439e-01,  1.1304e-02, -1.2319e-01, -2.8470e-01],\n",
       "                      [ 1.1873e-01,  4.4490e-01,  1.5813e-01, -3.2549e-01],\n",
       "                      [ 5.3854e-02,  3.0815e-01, -3.3580e-01, -3.7902e-01],\n",
       "                      [-4.1875e-01,  6.8740e-02,  4.0661e-01,  5.3741e-01],\n",
       "                      [ 2.9927e-01, -4.3159e-01,  6.9453e-02, -4.0000e-01],\n",
       "                      [-2.3878e-01, -3.6281e-01,  8.8211e-02, -4.7392e-01],\n",
       "                      [-1.5880e-01, -2.3821e-01,  4.3534e-01,  1.7904e-01],\n",
       "                      [ 2.3178e-01,  3.2165e-01,  4.3184e-01,  5.3715e-01],\n",
       "                      [-1.7094e-01,  2.4767e-01, -1.8797e-01, -2.9020e-01],\n",
       "                      [ 2.0385e-01,  4.0817e-01,  1.0693e-01,  7.7504e-02],\n",
       "                      [-1.7816e-02,  5.1781e-02, -1.1037e-01,  1.2595e-01],\n",
       "                      [ 3.5536e-01,  4.0224e-01,  4.0976e-01, -3.7184e-01],\n",
       "                      [-7.1496e-02,  2.0576e-01,  6.4674e-02, -2.9295e-01],\n",
       "                      [-3.6876e-01, -2.7450e-01, -4.3587e-01, -4.6635e-01],\n",
       "                      [-1.6100e-01, -3.8330e-01,  2.1469e-01, -4.5048e-01],\n",
       "                      [ 3.3387e-01,  4.5298e-02,  3.3702e-01, -4.6161e-01],\n",
       "                      [-5.1534e-02, -5.5767e-02, -5.4067e-01,  2.2255e-02],\n",
       "                      [ 1.4044e-01, -2.6676e-01,  3.6121e-01, -1.9389e-01],\n",
       "                      [ 2.9333e-01,  1.0964e-01, -1.9254e-01, -2.7034e-01],\n",
       "                      [ 1.1967e-01, -2.6610e-01, -4.0191e-01, -2.1795e-01],\n",
       "                      [ 1.7750e-01,  7.8838e-02, -6.0623e-02,  5.5613e-01],\n",
       "                      [-2.7706e-01, -4.1917e-01, -8.1165e-02,  5.2059e-01],\n",
       "                      [ 3.0755e-01, -2.3900e-01,  3.3670e-01, -2.2719e-01],\n",
       "                      [ 4.2825e-01,  2.6764e-01, -3.5182e-01, -3.7008e-01],\n",
       "                      [-4.6533e-01,  1.5071e-01,  1.8813e-01,  4.8544e-01],\n",
       "                      [ 4.1647e-01, -5.2436e-01, -1.2115e-01,  1.8841e-01],\n",
       "                      [-4.7393e-01,  2.1804e-01,  1.2357e-01,  3.2549e-01],\n",
       "                      [-1.1700e-01, -1.9066e-01, -3.5155e-01, -2.6133e-01],\n",
       "                      [-4.2342e-01, -6.8712e-02, -2.5296e-01, -3.8988e-01],\n",
       "                      [ 3.3596e-01,  4.9161e-01,  4.2986e-01,  4.5461e-01],\n",
       "                      [ 1.8180e-01, -3.7084e-02,  2.8332e-01,  2.4586e-01],\n",
       "                      [-1.4710e-01, -4.1969e-01, -8.4438e-02,  8.7955e-02],\n",
       "                      [-7.2012e-02, -6.8465e-02, -3.4191e-01,  4.4582e-01],\n",
       "                      [-2.5656e-01,  4.3012e-01, -3.2003e-01,  5.0930e-01],\n",
       "                      [ 3.7843e-01,  2.7320e-01, -1.0613e-01,  2.8692e-01],\n",
       "                      [-3.9675e-02,  2.1467e-01, -2.6634e-02,  4.0904e-01],\n",
       "                      [-2.9734e-01,  1.0982e-01, -1.4804e-01,  4.0654e-01],\n",
       "                      [ 2.8370e-02, -3.8588e-01,  3.5979e-01, -3.6858e-01],\n",
       "                      [-2.3769e-01,  4.4754e-01,  6.0873e-02, -1.0615e-01],\n",
       "                      [ 1.5510e-01,  1.1900e-01, -2.8504e-01, -2.8856e-02],\n",
       "                      [-1.6551e-02,  1.2428e-02,  4.3117e-01,  5.4137e-01],\n",
       "                      [-9.8183e-02, -2.5237e-01,  1.2073e-01, -3.1936e-01],\n",
       "                      [ 1.4213e-01, -7.6649e-02,  1.3719e-01, -6.2470e-02],\n",
       "                      [-7.6803e-02,  3.2445e-01, -3.6111e-02, -6.3827e-02],\n",
       "                      [-1.4312e-01, -2.6280e-01, -3.2686e-01,  3.7899e-01],\n",
       "                      [-4.0708e-01, -4.3703e-01,  2.1026e-01,  4.0734e-01],\n",
       "                      [ 3.1431e-01,  9.9267e-02,  1.0035e-01, -5.2973e-01],\n",
       "                      [ 6.1234e-02, -5.0773e-01,  4.3052e-01,  3.9668e-01],\n",
       "                      [-4.8332e-01,  2.3295e-01,  1.7954e-03, -3.9957e-01],\n",
       "                      [ 1.7226e-01, -3.8056e-01, -5.0726e-01, -3.0658e-01],\n",
       "                      [ 3.8245e-02, -1.3830e-01,  2.4880e-01,  4.7659e-01],\n",
       "                      [-2.2192e-01,  5.4935e-02, -9.0426e-02, -1.2057e-02],\n",
       "                      [-3.4690e-01,  5.6128e-02, -1.4140e-01, -2.9775e-01],\n",
       "                      [ 3.3280e-01,  3.4254e-01, -4.5726e-01,  1.9346e-01],\n",
       "                      [-3.1412e-01,  1.3523e-01,  7.4452e-02,  4.6265e-01],\n",
       "                      [ 3.4344e-02, -4.5089e-02, -6.3442e-02, -2.1258e-01],\n",
       "                      [-4.0104e-01,  3.7676e-01,  2.6124e-02, -3.5147e-01],\n",
       "                      [-4.4289e-01,  1.9697e-01, -2.3015e-01,  3.8354e-01],\n",
       "                      [ 4.4897e-01,  4.2889e-01,  3.7127e-02,  4.3793e-01],\n",
       "                      [-1.1007e-01,  4.6507e-03,  7.9117e-03,  3.5460e-01],\n",
       "                      [ 4.1372e-02, -3.3268e-01, -3.1218e-01, -3.7850e-01],\n",
       "                      [ 3.1979e-01, -2.8915e-01, -4.0909e-01,  2.8205e-01],\n",
       "                      [-1.6828e-01, -8.2628e-02,  4.8515e-01,  5.2192e-01],\n",
       "                      [ 1.3173e-01,  3.2591e-01,  1.0308e-01,  1.5257e-01],\n",
       "                      [ 1.0818e-01, -9.0525e-02, -1.0993e-01,  2.8432e-01],\n",
       "                      [-3.1949e-01,  1.7900e-01, -1.6595e-01,  1.4286e-01],\n",
       "                      [-3.1528e-01,  4.3637e-01, -2.9546e-01,  1.9785e-01],\n",
       "                      [-2.3535e-01, -1.7540e-01, -1.5241e-01,  4.2336e-01],\n",
       "                      [ 3.7640e-02, -1.6052e-01, -2.8855e-01, -4.4916e-01],\n",
       "                      [ 3.9370e-01,  3.6924e-01, -2.9381e-02, -5.0799e-01],\n",
       "                      [-4.5758e-01, -2.0048e-01,  4.5408e-01, -1.6089e-01],\n",
       "                      [-2.1708e-01, -1.7800e-01,  9.3957e-02,  3.3660e-01],\n",
       "                      [-3.4541e-01,  1.3293e-01,  4.5375e-01,  4.2582e-01],\n",
       "                      [ 2.3275e-01, -4.4847e-01,  3.9733e-01,  1.7836e-04],\n",
       "                      [ 1.8537e-01, -4.5013e-01, -2.2472e-01, -7.9352e-02],\n",
       "                      [-2.8622e-01,  4.1059e-01,  4.2896e-01,  2.2660e-01],\n",
       "                      [ 2.7403e-01, -7.8123e-02, -3.4802e-02, -1.6236e-01],\n",
       "                      [-5.3245e-01,  3.8095e-01, -3.2171e-01,  4.4497e-01],\n",
       "                      [ 7.6870e-02,  3.9735e-01, -3.6902e-01, -4.1627e-01],\n",
       "                      [ 6.9987e-02,  4.9272e-01,  2.0824e-01, -1.3177e-01],\n",
       "                      [-2.5574e-01, -1.8563e-01, -1.6079e-02,  4.0131e-02],\n",
       "                      [-1.2154e-01,  3.3583e-01, -4.0438e-01, -4.8737e-02],\n",
       "                      [-3.5682e-01, -4.5459e-01, -3.8579e-01,  4.7641e-01],\n",
       "                      [-1.0368e-01,  1.4565e-01, -4.3902e-01,  3.5384e-01],\n",
       "                      [ 1.1925e-01, -3.8948e-01, -5.4966e-01, -4.3802e-01],\n",
       "                      [ 3.0422e-01, -1.6352e-01, -3.1193e-01, -4.5649e-01],\n",
       "                      [-3.0068e-01,  5.1838e-02, -1.1599e-01, -4.4538e-01],\n",
       "                      [-2.9814e-01,  3.8808e-01, -3.7356e-01, -2.3852e-01],\n",
       "                      [-6.9696e-02,  2.0775e-01, -1.6601e-02, -2.6439e-01],\n",
       "                      [-1.8362e-01,  3.6535e-01, -2.9208e-01, -3.5347e-01],\n",
       "                      [ 9.3110e-02,  3.8012e-01, -3.5872e-01, -2.8454e-01],\n",
       "                      [ 4.4489e-01, -1.2581e-01,  2.0651e-01, -6.0107e-02],\n",
       "                      [ 3.6019e-01,  4.1114e-01, -3.9313e-01,  1.3815e-01],\n",
       "                      [-7.8702e-03, -4.5555e-01, -4.8333e-01,  1.2881e-01],\n",
       "                      [ 3.7257e-01, -5.7838e-02, -2.6811e-01, -3.5099e-01],\n",
       "                      [-4.0334e-01, -1.5630e-01, -2.5807e-01,  3.3839e-01],\n",
       "                      [ 1.9952e-01, -4.6344e-01, -2.8195e-01,  1.8425e-02],\n",
       "                      [-1.8483e-01, -1.7366e-01, -1.5672e-01, -1.0717e-01],\n",
       "                      [ 2.9376e-02,  3.0087e-01,  2.5494e-01, -2.6794e-01],\n",
       "                      [-1.8763e-01, -3.0085e-01, -2.0857e-01,  1.6244e-01],\n",
       "                      [-2.3661e-01, -2.1882e-02,  4.6294e-01,  4.4604e-03],\n",
       "                      [ 3.0527e-01, -2.2925e-01, -1.3682e-01, -2.6834e-01],\n",
       "                      [-3.4828e-01,  4.5931e-01, -1.4146e-01,  5.6906e-02],\n",
       "                      [ 1.9807e-01, -3.5899e-01, -4.3880e-01, -2.2573e-03],\n",
       "                      [-1.1478e-01, -4.2662e-01, -2.4183e-01, -1.3770e-01],\n",
       "                      [-1.7606e-01, -2.3255e-01,  2.3117e-02,  3.3319e-01],\n",
       "                      [-1.0990e-01, -1.8519e-01, -3.8362e-01,  1.3850e-01],\n",
       "                      [ 4.3753e-01, -1.4761e-01,  3.7949e-01,  2.9224e-01],\n",
       "                      [-2.9714e-01, -1.7489e-01, -2.5683e-01, -2.4189e-01]])),\n",
       "             ('policy.layers.0.bias',\n",
       "              tensor([-0.3964,  0.0042, -0.2864,  0.4536, -0.2031, -0.4766,  0.4824, -0.2411,\n",
       "                       0.4354,  0.4288,  0.1973,  0.2015, -0.2640,  0.1640, -0.2137, -0.4804,\n",
       "                       0.1175, -0.3377, -0.2321,  0.1130, -0.3671,  0.2991,  0.1702, -0.3533,\n",
       "                      -0.2014,  0.4488,  0.4751,  0.3085, -0.4319,  0.3182,  0.1752,  0.0243,\n",
       "                       0.4285, -0.3265, -0.0699,  0.2778,  0.1271,  0.3186,  0.4652,  0.3132,\n",
       "                       0.3732, -0.2541,  0.0977,  0.3408,  0.4099,  0.0457, -0.4648, -0.2345,\n",
       "                      -0.2606, -0.3995,  0.1153, -0.0124,  0.1302,  0.0521, -0.2924,  0.1925,\n",
       "                       0.3184,  0.3872, -0.0445, -0.1123, -0.3291, -0.4658,  0.2515, -0.5005,\n",
       "                       0.4726,  0.2450,  0.0882, -0.0865,  0.0667,  0.1823,  0.3746, -0.1156,\n",
       "                      -0.2945,  0.2401, -0.1761,  0.4693, -0.2656, -0.2132, -0.0165, -0.0871,\n",
       "                       0.1444,  0.4151, -0.4815,  0.3602,  0.0695, -0.3313,  0.1581,  0.1196,\n",
       "                      -0.3243,  0.4091, -0.2221,  0.0940,  0.3701, -0.3042,  0.4908,  0.0188,\n",
       "                       0.2541, -0.1077,  0.2346,  0.4871, -0.4932,  0.1914, -0.4862, -0.0383,\n",
       "                      -0.0149, -0.3879, -0.1739,  0.1306,  0.4170, -0.1945,  0.0820,  0.2037,\n",
       "                      -0.3066, -0.3649,  0.0096, -0.1477,  0.1297,  0.0138,  0.0897,  0.1903,\n",
       "                      -0.0822, -0.3795,  0.4430,  0.4173,  0.0212, -0.4400, -0.2060,  0.0779])),\n",
       "             ('policy.layers.2.weight',\n",
       "              tensor([[ 0.0369, -0.0493, -0.0525,  0.0303, -0.0560, -0.0321,  0.0699,  0.0024,\n",
       "                       -0.1048, -0.0332,  0.1061, -0.1110,  0.0201,  0.1251, -0.0780,  0.0797,\n",
       "                        0.0868,  0.0141, -0.0672, -0.0223,  0.0158,  0.0193, -0.0691, -0.0262,\n",
       "                       -0.0037, -0.0414,  0.0233,  0.0201,  0.0609, -0.0003, -0.0270, -0.0517,\n",
       "                        0.0841, -0.0459,  0.0735,  0.0329,  0.0010, -0.0032,  0.1018,  0.0072,\n",
       "                       -0.0197, -0.0419,  0.0969, -0.0010, -0.0667, -0.0324,  0.0793, -0.0601,\n",
       "                       -0.0788,  0.0413, -0.0669, -0.0816, -0.1033, -0.0666,  0.0181, -0.0627,\n",
       "                        0.0982, -0.0226, -0.0388, -0.0959,  0.0810, -0.0489, -0.0188,  0.0112,\n",
       "                        0.0433,  0.1020,  0.0116,  0.0524,  0.1331, -0.0082, -0.0099, -0.0092,\n",
       "                       -0.0076, -0.0109,  0.0379,  0.0722,  0.0207,  0.0375, -0.0954,  0.0779,\n",
       "                       -0.0670, -0.0465,  0.0749,  0.0457, -0.0527, -0.0414, -0.1156,  0.0032,\n",
       "                        0.0915, -0.0181,  0.0240, -0.0627, -0.0803, -0.0663, -0.0685, -0.0445,\n",
       "                        0.0073,  0.0313,  0.0747, -0.0494,  0.0612, -0.0554, -0.0730,  0.1171,\n",
       "                        0.1001,  0.0295, -0.0026,  0.1034,  0.1044,  0.0416, -0.0553,  0.0285,\n",
       "                       -0.0729, -0.0682,  0.0375,  0.0086, -0.0219,  0.0766, -0.0890,  0.0610,\n",
       "                        0.0214,  0.0578,  0.0495,  0.0546,  0.0386, -0.0574,  0.0211,  0.1490],\n",
       "                      [-0.0665,  0.0787, -0.0724, -0.0026,  0.0266, -0.0062, -0.0621, -0.0234,\n",
       "                        0.0394,  0.0740, -0.0343,  0.0248,  0.0378,  0.0165, -0.0803,  0.0329,\n",
       "                       -0.0599, -0.0552, -0.0729, -0.0990,  0.0665,  0.0476,  0.0529, -0.0504,\n",
       "                        0.0159,  0.0762,  0.0783,  0.0323,  0.0414,  0.0222,  0.0321, -0.1109,\n",
       "                       -0.1105, -0.0240, -0.0590, -0.0689,  0.0155, -0.0554, -0.0101,  0.1023,\n",
       "                        0.0361,  0.0621,  0.0245,  0.1172,  0.0089,  0.0865,  0.0078,  0.0494,\n",
       "                       -0.0006, -0.0254, -0.0504, -0.0235, -0.0032,  0.0064, -0.0655,  0.0190,\n",
       "                        0.0279, -0.0286, -0.0183, -0.0173,  0.0537,  0.0190, -0.0759,  0.0084,\n",
       "                       -0.0128, -0.0478,  0.0904,  0.0544, -0.0195,  0.1000,  0.0029,  0.0389,\n",
       "                        0.0920, -0.0303, -0.0596, -0.0642,  0.0754,  0.0447,  0.0336, -0.0488,\n",
       "                        0.0130,  0.1049,  0.0085,  0.0514,  0.0317,  0.0834, -0.0277,  0.0073,\n",
       "                       -0.0881,  0.0576, -0.0333, -0.0195,  0.0132,  0.0233,  0.0166,  0.0011,\n",
       "                        0.0664,  0.0059,  0.0142, -0.0261, -0.0046, -0.0201,  0.0535, -0.0766,\n",
       "                        0.0234, -0.0592, -0.1071, -0.0971,  0.0240,  0.0408,  0.0333, -0.0475,\n",
       "                        0.0431, -0.0121,  0.0906, -0.0577, -0.0883,  0.0034, -0.0344,  0.0677,\n",
       "                       -0.0051, -0.0379, -0.0506, -0.0336, -0.0225, -0.0432,  0.0231, -0.0075]])),\n",
       "             ('policy.layers.2.bias', tensor([0.0741, 0.0585]))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('policy.layers.0.weight',\n",
       "              tensor([[ 4.3611e-01, -2.0396e-01,  3.9874e-02,  1.3420e-01],\n",
       "                      [ 2.8729e-02,  2.1325e-01, -5.0081e-03,  2.3193e-01],\n",
       "                      [ 4.4421e-01, -1.3185e-01, -4.3520e-01, -9.0893e-02],\n",
       "                      [ 4.3958e-01, -4.1482e-01, -2.6271e-01,  3.3908e-01],\n",
       "                      [-2.9901e-01,  2.2829e-01,  9.9549e-02,  2.8231e-01],\n",
       "                      [ 3.2525e-01, -6.7828e-02,  1.6278e-01, -4.3135e-01],\n",
       "                      [-3.5815e-01,  3.9466e-02, -2.6664e-01,  2.2111e-01],\n",
       "                      [ 4.8813e-02,  1.6742e-02,  3.5585e-02, -3.1229e-01],\n",
       "                      [-4.8922e-01, -5.0620e-01,  3.1691e-02,  2.2765e-01],\n",
       "                      [ 1.6877e-01, -4.4479e-01, -1.5556e-01, -4.8567e-02],\n",
       "                      [ 2.4891e-01,  3.2433e-01, -3.6835e-01, -1.7808e-02],\n",
       "                      [-1.5222e-01,  1.0982e-01, -2.9689e-01,  3.0166e-01],\n",
       "                      [-4.2553e-01,  3.8729e-01,  4.7464e-01,  2.3338e-01],\n",
       "                      [ 1.9807e-01, -1.8616e-01, -5.3848e-01, -1.9335e-01],\n",
       "                      [ 1.3777e-01, -2.9924e-01,  2.7423e-01, -1.0082e-01],\n",
       "                      [ 2.9241e-01, -3.2148e-01,  4.8942e-01, -1.3820e-01],\n",
       "                      [-8.2829e-02, -7.2250e-02,  1.0225e-01, -1.0163e-01],\n",
       "                      [ 4.1562e-01,  2.2538e-01, -1.3217e-01,  3.9419e-01],\n",
       "                      [-4.4348e-01, -1.6705e-01,  7.2327e-02, -6.5985e-02],\n",
       "                      [-2.8439e-01,  1.1304e-02, -1.2319e-01, -2.8470e-01],\n",
       "                      [ 1.1873e-01,  4.4490e-01,  1.5813e-01, -3.2549e-01],\n",
       "                      [ 5.3854e-02,  3.0815e-01, -3.3580e-01, -3.7902e-01],\n",
       "                      [-4.1875e-01,  6.8740e-02,  4.0661e-01,  5.3741e-01],\n",
       "                      [ 2.9927e-01, -4.3159e-01,  6.9453e-02, -4.0000e-01],\n",
       "                      [-2.3878e-01, -3.6281e-01,  8.8211e-02, -4.7392e-01],\n",
       "                      [-1.5880e-01, -2.3821e-01,  4.3534e-01,  1.7904e-01],\n",
       "                      [ 2.3178e-01,  3.2165e-01,  4.3184e-01,  5.3715e-01],\n",
       "                      [-1.7094e-01,  2.4767e-01, -1.8797e-01, -2.9020e-01],\n",
       "                      [ 2.0385e-01,  4.0817e-01,  1.0693e-01,  7.7504e-02],\n",
       "                      [-1.7816e-02,  5.1781e-02, -1.1037e-01,  1.2595e-01],\n",
       "                      [ 3.5536e-01,  4.0224e-01,  4.0976e-01, -3.7184e-01],\n",
       "                      [-7.1496e-02,  2.0576e-01,  6.4674e-02, -2.9295e-01],\n",
       "                      [-3.6876e-01, -2.7450e-01, -4.3587e-01, -4.6635e-01],\n",
       "                      [-1.6100e-01, -3.8330e-01,  2.1469e-01, -4.5048e-01],\n",
       "                      [ 3.3387e-01,  4.5298e-02,  3.3702e-01, -4.6161e-01],\n",
       "                      [-5.1534e-02, -5.5767e-02, -5.4067e-01,  2.2255e-02],\n",
       "                      [ 1.4044e-01, -2.6676e-01,  3.6121e-01, -1.9389e-01],\n",
       "                      [ 2.9333e-01,  1.0964e-01, -1.9254e-01, -2.7034e-01],\n",
       "                      [ 1.1967e-01, -2.6610e-01, -4.0191e-01, -2.1795e-01],\n",
       "                      [ 1.7750e-01,  7.8838e-02, -6.0623e-02,  5.5613e-01],\n",
       "                      [-2.7706e-01, -4.1917e-01, -8.1165e-02,  5.2059e-01],\n",
       "                      [ 3.0755e-01, -2.3900e-01,  3.3670e-01, -2.2719e-01],\n",
       "                      [ 4.2825e-01,  2.6764e-01, -3.5182e-01, -3.7008e-01],\n",
       "                      [-4.6533e-01,  1.5071e-01,  1.8813e-01,  4.8544e-01],\n",
       "                      [ 4.1647e-01, -5.2436e-01, -1.2115e-01,  1.8841e-01],\n",
       "                      [-4.7393e-01,  2.1804e-01,  1.2357e-01,  3.2549e-01],\n",
       "                      [-1.1700e-01, -1.9066e-01, -3.5155e-01, -2.6133e-01],\n",
       "                      [-4.2342e-01, -6.8712e-02, -2.5296e-01, -3.8988e-01],\n",
       "                      [ 3.3596e-01,  4.9161e-01,  4.2986e-01,  4.5461e-01],\n",
       "                      [ 1.8180e-01, -3.7084e-02,  2.8332e-01,  2.4586e-01],\n",
       "                      [-1.4710e-01, -4.1969e-01, -8.4438e-02,  8.7955e-02],\n",
       "                      [-7.2012e-02, -6.8465e-02, -3.4191e-01,  4.4582e-01],\n",
       "                      [-2.5656e-01,  4.3012e-01, -3.2003e-01,  5.0930e-01],\n",
       "                      [ 3.7843e-01,  2.7320e-01, -1.0613e-01,  2.8692e-01],\n",
       "                      [-3.9675e-02,  2.1467e-01, -2.6634e-02,  4.0904e-01],\n",
       "                      [-2.9734e-01,  1.0982e-01, -1.4804e-01,  4.0654e-01],\n",
       "                      [ 2.8370e-02, -3.8588e-01,  3.5979e-01, -3.6858e-01],\n",
       "                      [-2.3769e-01,  4.4754e-01,  6.0873e-02, -1.0615e-01],\n",
       "                      [ 1.5510e-01,  1.1900e-01, -2.8504e-01, -2.8856e-02],\n",
       "                      [-1.6551e-02,  1.2428e-02,  4.3117e-01,  5.4137e-01],\n",
       "                      [-9.8183e-02, -2.5237e-01,  1.2073e-01, -3.1936e-01],\n",
       "                      [ 1.4213e-01, -7.6649e-02,  1.3719e-01, -6.2470e-02],\n",
       "                      [-7.6803e-02,  3.2445e-01, -3.6111e-02, -6.3827e-02],\n",
       "                      [-1.4312e-01, -2.6280e-01, -3.2686e-01,  3.7899e-01],\n",
       "                      [-4.0708e-01, -4.3703e-01,  2.1026e-01,  4.0734e-01],\n",
       "                      [ 3.1431e-01,  9.9267e-02,  1.0035e-01, -5.2973e-01],\n",
       "                      [ 6.1234e-02, -5.0773e-01,  4.3052e-01,  3.9668e-01],\n",
       "                      [-4.8332e-01,  2.3295e-01,  1.7954e-03, -3.9957e-01],\n",
       "                      [ 1.7226e-01, -3.8056e-01, -5.0726e-01, -3.0658e-01],\n",
       "                      [ 3.8245e-02, -1.3830e-01,  2.4880e-01,  4.7659e-01],\n",
       "                      [-2.2192e-01,  5.4935e-02, -9.0426e-02, -1.2057e-02],\n",
       "                      [-3.4690e-01,  5.6128e-02, -1.4140e-01, -2.9775e-01],\n",
       "                      [ 3.3280e-01,  3.4254e-01, -4.5726e-01,  1.9346e-01],\n",
       "                      [-3.1412e-01,  1.3523e-01,  7.4452e-02,  4.6265e-01],\n",
       "                      [ 3.4344e-02, -4.5089e-02, -6.3442e-02, -2.1258e-01],\n",
       "                      [-4.0104e-01,  3.7676e-01,  2.6124e-02, -3.5147e-01],\n",
       "                      [-4.4289e-01,  1.9697e-01, -2.3015e-01,  3.8354e-01],\n",
       "                      [ 4.4897e-01,  4.2889e-01,  3.7127e-02,  4.3793e-01],\n",
       "                      [-1.1007e-01,  4.6507e-03,  7.9117e-03,  3.5460e-01],\n",
       "                      [ 4.1372e-02, -3.3268e-01, -3.1218e-01, -3.7850e-01],\n",
       "                      [ 3.1979e-01, -2.8915e-01, -4.0909e-01,  2.8205e-01],\n",
       "                      [-1.6828e-01, -8.2628e-02,  4.8515e-01,  5.2192e-01],\n",
       "                      [ 1.3173e-01,  3.2591e-01,  1.0308e-01,  1.5257e-01],\n",
       "                      [ 1.0818e-01, -9.0525e-02, -1.0993e-01,  2.8432e-01],\n",
       "                      [-3.1949e-01,  1.7900e-01, -1.6595e-01,  1.4286e-01],\n",
       "                      [-3.1528e-01,  4.3637e-01, -2.9546e-01,  1.9785e-01],\n",
       "                      [-2.3535e-01, -1.7540e-01, -1.5241e-01,  4.2336e-01],\n",
       "                      [ 3.7640e-02, -1.6052e-01, -2.8855e-01, -4.4916e-01],\n",
       "                      [ 3.9370e-01,  3.6924e-01, -2.9381e-02, -5.0799e-01],\n",
       "                      [-4.5758e-01, -2.0048e-01,  4.5408e-01, -1.6089e-01],\n",
       "                      [-2.1708e-01, -1.7800e-01,  9.3957e-02,  3.3660e-01],\n",
       "                      [-3.4541e-01,  1.3293e-01,  4.5375e-01,  4.2582e-01],\n",
       "                      [ 2.3275e-01, -4.4847e-01,  3.9733e-01,  1.7836e-04],\n",
       "                      [ 1.8537e-01, -4.5013e-01, -2.2472e-01, -7.9352e-02],\n",
       "                      [-2.8622e-01,  4.1059e-01,  4.2896e-01,  2.2660e-01],\n",
       "                      [ 2.7403e-01, -7.8123e-02, -3.4802e-02, -1.6236e-01],\n",
       "                      [-5.3245e-01,  3.8095e-01, -3.2171e-01,  4.4497e-01],\n",
       "                      [ 7.6870e-02,  3.9735e-01, -3.6902e-01, -4.1627e-01],\n",
       "                      [ 6.9987e-02,  4.9272e-01,  2.0824e-01, -1.3177e-01],\n",
       "                      [-2.5574e-01, -1.8563e-01, -1.6079e-02,  4.0131e-02],\n",
       "                      [-1.2154e-01,  3.3583e-01, -4.0438e-01, -4.8737e-02],\n",
       "                      [-3.5682e-01, -4.5459e-01, -3.8579e-01,  4.7641e-01],\n",
       "                      [-1.0368e-01,  1.4565e-01, -4.3902e-01,  3.5384e-01],\n",
       "                      [ 1.1925e-01, -3.8948e-01, -5.4966e-01, -4.3802e-01],\n",
       "                      [ 3.0422e-01, -1.6352e-01, -3.1193e-01, -4.5649e-01],\n",
       "                      [-3.0068e-01,  5.1838e-02, -1.1599e-01, -4.4538e-01],\n",
       "                      [-2.9814e-01,  3.8808e-01, -3.7356e-01, -2.3852e-01],\n",
       "                      [-6.9696e-02,  2.0775e-01, -1.6601e-02, -2.6439e-01],\n",
       "                      [-1.8362e-01,  3.6535e-01, -2.9208e-01, -3.5347e-01],\n",
       "                      [ 9.3110e-02,  3.8012e-01, -3.5872e-01, -2.8454e-01],\n",
       "                      [ 4.4489e-01, -1.2581e-01,  2.0651e-01, -6.0107e-02],\n",
       "                      [ 3.6019e-01,  4.1114e-01, -3.9313e-01,  1.3815e-01],\n",
       "                      [-7.8702e-03, -4.5555e-01, -4.8333e-01,  1.2881e-01],\n",
       "                      [ 3.7257e-01, -5.7838e-02, -2.6811e-01, -3.5099e-01],\n",
       "                      [-4.0334e-01, -1.5630e-01, -2.5807e-01,  3.3839e-01],\n",
       "                      [ 1.9952e-01, -4.6344e-01, -2.8195e-01,  1.8425e-02],\n",
       "                      [-1.8483e-01, -1.7366e-01, -1.5672e-01, -1.0717e-01],\n",
       "                      [ 2.9376e-02,  3.0087e-01,  2.5494e-01, -2.6794e-01],\n",
       "                      [-1.8763e-01, -3.0085e-01, -2.0857e-01,  1.6244e-01],\n",
       "                      [-2.3661e-01, -2.1882e-02,  4.6294e-01,  4.4604e-03],\n",
       "                      [ 3.0527e-01, -2.2925e-01, -1.3682e-01, -2.6834e-01],\n",
       "                      [-3.4828e-01,  4.5931e-01, -1.4146e-01,  5.6906e-02],\n",
       "                      [ 1.9807e-01, -3.5899e-01, -4.3880e-01, -2.2573e-03],\n",
       "                      [-1.1478e-01, -4.2662e-01, -2.4183e-01, -1.3770e-01],\n",
       "                      [-1.7606e-01, -2.3255e-01,  2.3117e-02,  3.3319e-01],\n",
       "                      [-1.0990e-01, -1.8519e-01, -3.8362e-01,  1.3850e-01],\n",
       "                      [ 4.3753e-01, -1.4761e-01,  3.7949e-01,  2.9224e-01],\n",
       "                      [-2.9714e-01, -1.7489e-01, -2.5683e-01, -2.4189e-01]])),\n",
       "             ('policy.layers.0.bias',\n",
       "              tensor([-0.3964,  0.0042, -0.2864,  0.4536, -0.2031, -0.4766,  0.4824, -0.2411,\n",
       "                       0.4354,  0.4288,  0.1973,  0.2015, -0.2640,  0.1640, -0.2137, -0.4804,\n",
       "                       0.1175, -0.3377, -0.2321,  0.1130, -0.3671,  0.2991,  0.1702, -0.3533,\n",
       "                      -0.2014,  0.4488,  0.4751,  0.3085, -0.4319,  0.3182,  0.1752,  0.0243,\n",
       "                       0.4285, -0.3265, -0.0699,  0.2778,  0.1271,  0.3186,  0.4652,  0.3132,\n",
       "                       0.3732, -0.2541,  0.0977,  0.3408,  0.4099,  0.0457, -0.4648, -0.2345,\n",
       "                      -0.2606, -0.3995,  0.1153, -0.0124,  0.1302,  0.0521, -0.2924,  0.1925,\n",
       "                       0.3184,  0.3872, -0.0445, -0.1123, -0.3291, -0.4658,  0.2515, -0.5005,\n",
       "                       0.4726,  0.2450,  0.0882, -0.0865,  0.0667,  0.1823,  0.3746, -0.1156,\n",
       "                      -0.2945,  0.2401, -0.1761,  0.4693, -0.2656, -0.2132, -0.0165, -0.0871,\n",
       "                       0.1444,  0.4151, -0.4815,  0.3602,  0.0695, -0.3313,  0.1581,  0.1196,\n",
       "                      -0.3243,  0.4091, -0.2221,  0.0940,  0.3701, -0.3042,  0.4908,  0.0188,\n",
       "                       0.2541, -0.1077,  0.2346,  0.4871, -0.4932,  0.1914, -0.4862, -0.0383,\n",
       "                      -0.0149, -0.3879, -0.1739,  0.1306,  0.4170, -0.1945,  0.0820,  0.2037,\n",
       "                      -0.3066, -0.3649,  0.0096, -0.1477,  0.1297,  0.0138,  0.0897,  0.1903,\n",
       "                      -0.0822, -0.3795,  0.4430,  0.4173,  0.0212, -0.4400, -0.2060,  0.0779])),\n",
       "             ('policy.layers.2.weight',\n",
       "              tensor([[ 0.0369, -0.0493, -0.0525,  0.0303, -0.0560, -0.0321,  0.0699,  0.0024,\n",
       "                       -0.1048, -0.0332,  0.1061, -0.1110,  0.0201,  0.1251, -0.0780,  0.0797,\n",
       "                        0.0868,  0.0141, -0.0672, -0.0223,  0.0158,  0.0193, -0.0691, -0.0262,\n",
       "                       -0.0037, -0.0414,  0.0233,  0.0201,  0.0609, -0.0003, -0.0270, -0.0517,\n",
       "                        0.0841, -0.0459,  0.0735,  0.0329,  0.0010, -0.0032,  0.1018,  0.0072,\n",
       "                       -0.0197, -0.0419,  0.0969, -0.0010, -0.0667, -0.0324,  0.0793, -0.0601,\n",
       "                       -0.0788,  0.0413, -0.0669, -0.0816, -0.1033, -0.0666,  0.0181, -0.0627,\n",
       "                        0.0982, -0.0226, -0.0388, -0.0959,  0.0810, -0.0489, -0.0188,  0.0112,\n",
       "                        0.0433,  0.1020,  0.0116,  0.0524,  0.1331, -0.0082, -0.0099, -0.0092,\n",
       "                       -0.0076, -0.0109,  0.0379,  0.0722,  0.0207,  0.0375, -0.0954,  0.0779,\n",
       "                       -0.0670, -0.0465,  0.0749,  0.0457, -0.0527, -0.0414, -0.1156,  0.0032,\n",
       "                        0.0915, -0.0181,  0.0240, -0.0627, -0.0803, -0.0663, -0.0685, -0.0445,\n",
       "                        0.0073,  0.0313,  0.0747, -0.0494,  0.0612, -0.0554, -0.0730,  0.1171,\n",
       "                        0.1001,  0.0295, -0.0026,  0.1034,  0.1044,  0.0416, -0.0553,  0.0285,\n",
       "                       -0.0729, -0.0682,  0.0375,  0.0086, -0.0219,  0.0766, -0.0890,  0.0610,\n",
       "                        0.0214,  0.0578,  0.0495,  0.0546,  0.0386, -0.0574,  0.0211,  0.1490],\n",
       "                      [-0.0665,  0.0787, -0.0724, -0.0026,  0.0266, -0.0062, -0.0621, -0.0234,\n",
       "                        0.0394,  0.0740, -0.0343,  0.0248,  0.0378,  0.0165, -0.0803,  0.0329,\n",
       "                       -0.0599, -0.0552, -0.0729, -0.0990,  0.0665,  0.0476,  0.0529, -0.0504,\n",
       "                        0.0159,  0.0762,  0.0783,  0.0323,  0.0414,  0.0222,  0.0321, -0.1109,\n",
       "                       -0.1105, -0.0240, -0.0590, -0.0689,  0.0155, -0.0554, -0.0101,  0.1023,\n",
       "                        0.0361,  0.0621,  0.0245,  0.1172,  0.0089,  0.0865,  0.0078,  0.0494,\n",
       "                       -0.0006, -0.0254, -0.0504, -0.0235, -0.0032,  0.0064, -0.0655,  0.0190,\n",
       "                        0.0279, -0.0286, -0.0183, -0.0173,  0.0537,  0.0190, -0.0759,  0.0084,\n",
       "                       -0.0128, -0.0478,  0.0904,  0.0544, -0.0195,  0.1000,  0.0029,  0.0389,\n",
       "                        0.0920, -0.0303, -0.0596, -0.0642,  0.0754,  0.0447,  0.0336, -0.0488,\n",
       "                        0.0130,  0.1049,  0.0085,  0.0514,  0.0317,  0.0834, -0.0277,  0.0073,\n",
       "                       -0.0881,  0.0576, -0.0333, -0.0195,  0.0132,  0.0233,  0.0166,  0.0011,\n",
       "                        0.0664,  0.0059,  0.0142, -0.0261, -0.0046, -0.0201,  0.0535, -0.0766,\n",
       "                        0.0234, -0.0592, -0.1071, -0.0971,  0.0240,  0.0408,  0.0333, -0.0475,\n",
       "                        0.0431, -0.0121,  0.0906, -0.0577, -0.0883,  0.0034, -0.0344,  0.0677,\n",
       "                       -0.0051, -0.0379, -0.0506, -0.0336, -0.0225, -0.0432,  0.0231, -0.0075]])),\n",
       "             ('policy.layers.2.bias', tensor([0.0741, 0.0585]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent2.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wandb 를 활용해서 결과를 확인해봅시다.\n",
    "\n",
    "여러번의 run을 돌려서 batch update REINFORCE 실험들을 확인해볼까요?\n",
    "결과는 [여기](https://app.wandb.ai/junyoung-park/reinforce_exp?workspace=user-junyoung-park)에서 확인해볼수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
