{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..') # add project root to the python path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 마르코프 의사결정 과정 MDP\n",
    "\n",
    "MDP는 강화 학습문제를 정의하는 수학적 장치입니다. MDP는 일반적으로 $<\\cal{S}, \\cal{A}, P, R, \\gamma>$ 인 튜플로 정의됩니다.\n",
    "이번 실습에서는 MDP의 각 요소를 확인해봅시다!\n",
    "\n",
    "당분간 MDP의 예제로 `GridworldEnv`를 사용할 것입니다. `GridworldEnv`는 openai `gym` 의 `discrete.DiscreteEnv` 를 상속받아서 만들어졌습니다. openai `gym`은 강화학습 환경으로 사용하기에 적합한 표준화된 인터페이스를 제공하기 때문에, 많은 강화학습 환경들의 베이스 클래스로 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.gridworld import GridworldEnv # Gridworld Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_y, num_x = 4, 4\n",
    "env = GridworldEnv(shape=[num_y, num_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4 X 4 그리드월드` \n",
    "\n",
    "우리의 MDP 환경을 시각화하면 아래의 그림과 같습니다.\n",
    "\n",
    "```\n",
    "===========\n",
    "T  x  o  o\n",
    "o  o  o  o\n",
    "o  o  o  o\n",
    "o  o  o  T\n",
    "===========\n",
    "```\n",
    "\n",
    "T: 도착점 (종결상태, Terminal state) <br>\n",
    "x: 현재 위치 $s_t$<br>\n",
    "o: 다른 환경의 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상태공간 $\\cal{S}$ 와 행동공간 $\\cal{A}$\n",
    "\n",
    "그리드 월드에서 상태 공간 $\\cal{S}$은 가로 4개 세로 4개의 격자로 전체 16개의 상태가 존재합니다. <br>\n",
    "그리드 월드에서 행동 공간 $\\cal{A}$은 모든 상태 $s$ 에서 \n",
    "\n",
    ">`위로 움직이기`, `오른쪽으로 움직이기`, `아래로 움직이기`, `왼쪽으로 움직이기` \n",
    "\n",
    "로 4가지입니다. <br>\n",
    "\n",
    "만약 가장자리에서 바깥으로 나가는 `행동`을 하게되면, 에이전트의 위치는 바뀌지 않습니다. <br> \n",
    "예를들어, 우상단에서 `위로 움직이기` 혹은 `오른쪽으로 움직이기`를 선택해서 행동하게 되면 에이전트는 움직이지 않게됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: Discrete(16)\n",
      "Number of actions: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"Number of states: {}\".format(observation_space))\n",
    "print(\"Number of actions: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상태천이 행렬 $P$, 과 보상함수 $R$\n",
    "\n",
    "강의에서 설명했듯, MDP의 상태천이 행렬 $P$는 사실은 행렬이 아닙니다. 명확하게 이야기하면 상태천이 `텐서` 라고 지칭하는게 명확하지만 편의를 위해 앞으로는 상태천이 행렬 (Transition matrix) 혹은 상태천이 모델 (Transition matrix)등으로 지칭하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 Tensor\n",
    "\n",
    "<img src=\"./images/tensor.png\" width=\"100%\" height=\"50%\" title=\"px(픽셀) 크기 설정\" alt=\"Tensor\"></img>\n",
    "\n",
    "앞으로 우리는 `n`차원 자료형을 Rank `n` 텐서라고 혹은 `n`차원 텐서라고 부를 것 입니다. `n` 을 직관적으로 이해하면, '텐서내에 특정 자료에 접근하기\n",
    "위해서는 몇개의 인덱스가 필요한가' 라고 생각해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # 파이썬에서 계산과학 및 수학계산을 위해서 많이 사용되는 라이브러리인 `numpy`를 불러옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `import numpy as np` ?\n",
    "\n",
    "```python\n",
    "import numpy\n",
    "import numpy as npy \n",
    "```\n",
    "\n",
    "또한 모두 동일하게 작동합니다. 하지만 `numpy` 를 `np` 라고 임포트하는 것은\n",
    "많은 사람들 사이에 관용적 표현입니다. <br>\n",
    "\n",
    "따라서 아래의 경우도 __모두 잘 작동합니다__.\n",
    "\n",
    "```python\n",
    "import numpy as pd\n",
    "import torch as np\n",
    "import pandas as th\n",
    "```\n",
    "\n",
    "라고 임포트해도 사용상에는 아무 문제가 없지만 누군가가 여러분들의 코드를 보면\n",
    "아마도 여러분께 화를 낼지도 모릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row = 2\n",
    "num_col = 2 \n",
    "\n",
    "# [num_row x now_col] 행렬을 만듭니다. 이때 행렬의 각 원소는 임의의 값으로 채워집니다.\n",
    "rank2_tensor = np.random.random(size=(num_row,num_col)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서와 친해지기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30604985 0.89335383]\n",
      " [0.26838695 0.90272819]]\n"
     ]
    }
   ],
   "source": [
    "print(rank2_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy_array.shape()`은 각 차원별로 몇개의 인덱스가 있는지를 반환해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape : (2, 2)\n",
      "Tensor rank : 2\n"
     ]
    }
   ],
   "source": [
    "tesor_shape = rank2_tensor.shape\n",
    "tensor_rank = len(tesor_shape)\n",
    "print(\"Tensor shape : {}\".format(tesor_shape))\n",
    "print(\"Tensor rank : {}\".format(tensor_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상태천이 매트릭스 (텐서) $P$ \n",
    "\n",
    "상태천이 매트릭스 $P$는 랭크 3 텐서 입니다. 첫번째 축은 행동 $a$에 대한 축이며, 둘째 축은 현재상태 $s$, 셋째 축은 다음상태 $s'$를 나타냅니다.\n",
    "\n",
    "\n",
    "행동 $a$의 인덱스를 0,1,2,3로 주었으며, 각각 현재 위치에서 [상,우,하,좌] 로 움직이는 행동 입니다. <br>\n",
    "(왜 인덱스는 0부터시작? 파이썬은 인덱싱을 0부터 시작합니다.)\n",
    "\n",
    "상태 $s$의 종류 4x4=16개 입니다. 이에 따라 인덱스를 0,1,.., 15까지 주었습니다. 0은 좌측 상단 위치, 1은 좌상단에서 오른쪽으로 한칸, ... , 15는 우측 하단의 위치를 표현합니다. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드에서 상태천이 텐서  $P$ 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P shape : (4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "P = env.P_tensor\n",
    "print(\"P shape : {}\".format(P.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Numpy` 배열의 장점, 직관적인(?) 편리한 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동 '상'(인덱스0)을 선택했을때 현재상태에서 다음상태로 전이하는 확률을 나타냄.\n",
    "action_up_prob = P[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(action_up_prob) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridworldEnv` 는 행동에 대한 상태천이가 결정적(deteministic) 하게 디자인했습니다.\n",
    "\n",
    "\n",
    ">즉, 상태 천이 매트릭스 $P$ 의 각 열(row)의 원소가 하나를 제외하고 모두 0.0 입니다.\n",
    "\n",
    "\n",
    "이 경우 `상` 이라는 행동을 하면, 무조건 위로 움직이게 됩니다. 하지만 일반적인 MDP에서는\n",
    "에이전트의 행동이 결정적 (deterministic) 으로 환경에 영향을 미치지 않고 추계적으로(stochastic) 하게 영향을 미칩니다. 예를 들자면, `상` 이라는 행동을 했지만, 모종의 이유로 위로 가지않고 다른 방향으로 가게될 수도 있다는 것이죠.\n",
    "\n",
    ">추계적 (stochastic) = '랜덤적인 혹은 확률적인 요소를 가지고 있다' 는 의미\n",
    "\n",
    "하지만 결정적인 `GridworldEnv` 에서는 최적정책 및 최적가치를 직관적으로 이해할 수 있으니 당분간은\n",
    "결정적인 환경을 고려해보도록 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상태천이 행렬의 특징을 기억하세요?\n",
    "\n",
    "상태천이 행렬의 각 행(row)는 특정 상태 $s$에서 다음 상태 $s'$ 으로 이동할 확률을 나타냅니다.\n",
    "따라서 (1) 상태천이 행렬의 모든 원소의 값은 항상 0보다 크거나 같고 (2) 각 열의 원소의 합은 1이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 그럼 모든 원소가 0보다 클까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_up_prob >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 눈으로 확인도 좋지만, 코딩으로도 확인해보자\n",
    "\n",
    "우리는 매트릭스의 전체크기를 알고 있으므로 모든 원소가 양수인지를 코딩으로도 확인해볼 수 있습니다.\n",
    "\n",
    "#### 방어적 프로그래밍\n",
    "\n",
    "__운전에서도 방어운전이 제일이듯, 코딩도 방어적 프로그래밍이 제일입니다__ <br>\n",
    "\n",
    "학습 알고리즘이 학습 할때는 우리가 디자인하지 않은 상황이 발생해도 그냥 학습이 진행되는 경우가 빈번합니다. 가끔은 연산중에 의도치 않은 오류로 `Nan`=__N__ot __a__ __N__umber 이 발생해도 학습이 진행됩니다. 그리고 나서 최악의 경우에 며칠후에 학습이 끝난 모델을 검증할 때서야, \n",
    "\n",
    "> 오... 1일때 부터 심각한 문제가 있었구나!\n",
    "\n",
    "라는것을 알고 나서 울면서 다시 모델을 학습하는 경우들이 있습니다. <br>\n",
    "\n",
    "가능하다면 코딩을 하실때도 각 요소가 어떻게 행동할지 알고 있다면, 코드내에서 요소들이 의도한대로 작동하는지 꼭 확인해보시길 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_greater_than_0 = action_up_prob >= 0\n",
    "is_all_greater_than_0 = is_greater_than_0.sum() == is_greater_than_0.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_all_greater_than_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 가로합은 1.0 일까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_up_prob.sum(axis=1) # 2 번째 축으로 합을 계산합니다. 파이썬은 숫자를 0부터 셉니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소소한 코딩팁. `Numpy` 의 auto inferencing 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_up_prob.sum(axis=-1) # '마지막' 축으로 합을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보상함수 $R$ \n",
    "\n",
    "$R$ 는 Rank2 텐서, 즉 매트릭스입니다. 세로축은 상태를 가로축은 행동을 의미합니다.\n",
    "\n",
    "우리의 `GridworldEnv`는 종결점에 도달하기 전까지는 어떤 상태에서 어떤 행동을 하든 매 이동마다 `-1` 의 보상을 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "R = env.R_tensor\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP 의 Episode\n",
    "\n",
    "MDP의 Episode는 $<(s_0, a_0, r_0, s_1), (s_1, a_1, r_1, s_2), ..., (s_{t}, a_{t}, r_{t}, s_{t+1}),..., (s_{T-1}, a_{T-1}, r_{T-1}, s_{T})>$ 으로 구성됩니다. 각 튜플은 매 시점 t의 상태 $s_t$, 선택한 행동 $a_t$, 보상 $r_t$, 그 다음 상태 $s_{t+1}$ 를 포함합니다. $T$는 종결 시점을 나타냅니다. \n",
    "\n",
    "하지만 일반적으로 RL 알고리즘을 구현할때는 구현의 편의를 위해 특정 시점이 종결 상태인지 아닌지를 확인하는 `done` 이라는 정보를 episode의 각 튜플에 추가해서 관리합니다.\n",
    "\n",
    "`GridworldEnv`에서 에피소드를 한번 시뮬레이션 해보도록 하죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP를 시뮬레이션하기 위해서는 정책 $\\pi$ 가 필요합니다!\n",
    "\n",
    "일단은 각 상태에서 모든 행동을 0.25의 확률로 고르는 정책함수로 `Gridworld`를 시뮬레이션 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position index : 13\n"
     ]
    }
   ],
   "source": [
    "_ = env.reset() # Gridworld 를 초기화합니다.\n",
    "print(\"Current position index : {}\".format(env.s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좀 더 직관적인 가시화를 위해서 action 인덱스를 방향으로 바꿔줍니다.\n",
    "action_mapper = {\n",
    "    0: 'UP',\n",
    "    1: 'RIGHT',\n",
    "    2: 'DOWN',\n",
    "    3: 'LEFT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At t = 0\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  x  o  T\n",
      "==========\n",
      "state : 13\n",
      "aciton : RIGHT\n",
      "reward : -1.0\n",
      "next state : 14 \n",
      "\n",
      "At t = 1\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  x  T\n",
      "==========\n",
      "state : 14\n",
      "aciton : DOWN\n",
      "reward : -1.0\n",
      "next state : 14 \n",
      "\n",
      "At t = 2\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  x  T\n",
      "==========\n",
      "state : 14\n",
      "aciton : RIGHT\n",
      "reward : -1.0\n",
      "next state : 15 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "step_counter = 0\n",
    "while True:\n",
    "    # [0,4) 에서 정수값 1개를 임의로 선택합니다. 기본적인 설정이 high 값은 포함하지 않는다는 것에 유의!        \n",
    "    print(\"At t = {}\".format(step_counter))\n",
    "    env._render()\n",
    "    \n",
    "    cur_state = env.s\n",
    "    action = np.random.randint(low=0, high=4)\n",
    "    next_state, reward, done, info = env.step(action)    \n",
    "    \n",
    "    print(\"state : {}\".format(cur_state))\n",
    "    print(\"aciton : {}\".format(action_mapper[action]))\n",
    "    print(\"reward : {}\".format(reward))\n",
    "    print(\"next state : {} \\n\".format(next_state))\n",
    "    step_counter += 1\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여러 에피소드를 시뮬레이션 해보기\n",
    "\n",
    "`GridworldEnv` 의 상태천이 행렬 $P$ 가 결정적이지만, 정책함수 $\\pi$ 가 추계적이므로 같은 시작 상태에서 에피소드를 시작하더라도 각 에피소드에서 방문한 상태 및 에피소드의 길이가 다를 수 있습니다. 그또한 한번 시뮬레이션 해보도록하죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, s0):\n",
    "    _ = env.reset() # Gridworld 를 초기화합니다.\n",
    "    env.s = s0\n",
    "    \n",
    "    step_counter = 0\n",
    "    while True:\n",
    "        action = np.random.randint(low=0, high=4)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        step_counter += 1\n",
    "        if done:\n",
    "            break\n",
    "    return step_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Length of episode : 10\n",
      "Episode 1 | Length of episode : 7\n",
      "Episode 2 | Length of episode : 23\n",
      "Episode 3 | Length of episode : 28\n",
      "Episode 4 | Length of episode : 6\n",
      "Episode 5 | Length of episode : 5\n",
      "Episode 6 | Length of episode : 13\n",
      "Episode 7 | Length of episode : 20\n",
      "Episode 8 | Length of episode : 7\n",
      "Episode 9 | Length of episode : 21\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "s0 = 6\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    len_ep = run_episode(env, s0)\n",
    "    print(\"Episode {} | Length of episode : {}\".format(i, len_ep))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
