{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..') # add project root to the python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.toy_text import CliffWalkingEnv\n",
    "\n",
    "from src.part2.temporal_difference import SARSA, QLearner\n",
    "from src.common.grid_visualization import visualize_value_function, visualize_policy\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yoonju/anaconda3/envs/RL101/lib/python3.9/site-packages/gym/__init__.py'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "gym.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA와 Q-Learning\n",
    "\n",
    "SARSA 와 Q-learning 모두 TD기법을 활용하여 최적정책을 찾는 방법이었습니다. 하지만 이 두가지 알고리즘은 각각 on-policy 와 off-policy 라는 본질적인 차이를 가지고 있었던것 기억하시나요?\n",
    "\n",
    "> 1. on-policy 기법은 현재 평가하는 정책 $\\pi(a|s)$ 와 행동 정책 $\\mu(a|s)$이 동일\n",
    "> 2. off-policy 기법은 현재 평가하는 정책 $\\pi(a|s)$ 와 행동 정책 $\\mu(a|s)$이 다름\n",
    "\n",
    "즉 `Q-learning` 알고리즘은 환경으로부터 데이터를 얻기위해 사용했던 $\\epsilon$-탐욕적 정책 대신, 알고리즘의 성능을 평가하기 위해서, 현재 추산된 $Q(s,a)$로 탐욕적 정책을 사용할수 있다는 것입니다. 이번 실습에서는 새로운 환경에서 `SARSA`의 $\\epsilon$-탐욕적 정책과 `Q-learning`을 통해서 구한 $Q(s,a)$에 탐욕적 정책의 차이를 확인해볼까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CliffWalkingEnv` \n",
    " \n",
    "이번 실습에서는 새로운 환경인 `CliffWalkingEnv`를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliff_env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, {'prob': 1})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cliff_env.render_mode='ansi' #'human' 으로 바꾸면 UI로 보여줌\n",
    "cliff_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CliffWalkingEnv` 돌아보기\n",
    "\n",
    "`CliffWalkingEnv` 는 일종의 그리드 환경입니다. \n",
    "1. 상태공간 $\\cal{S}$은 가로로 12칸 세로로 4칸인 격자 공간으로서, 총 48개의 상태 $s$ 가 존재합니다. <br>\n",
    "2. 행동 공간 $\\cal{A}$은 모든 상태 $s$에 대해서 `'상','우','하','좌'`입니다. 즉 4개의 행동이 존재합니다. <br>\n",
    "3. 초기상태 $s_0$은 항상 좌측 하단 셀로 고정되어 있습니다. 종결상태는 가장 오른쪽 하단입니다. 절벽상태 $c$는 초기상태와 종결상태를 제외한 모든 최하단 셀들입니다. <br>\n",
    "4. 보상함수는 매 이동마다 `-1` 이며 추가적으로 절벽 $c$ 에 도달할 때마다 `-100`을 받습니다. <br>\n",
    "5. 만약 절벽상태 $c$ 에 도달하면 다시 초기상태 $s_0$로 되돌아가게 됩니다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cliff_env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA, Q-Learning 에이전트 초기화하기\n",
    "\n",
    "두개의 에이전트를 모두 동일한 파라미터로 초기화 하였습니다. $\\epsilon$ 의 경우 `적당하게` 낮은 0.1 정도로 설정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cliff_env.nS, cliff_env.nA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent = SARSA(gamma=.9,\n",
    "                    lr=1e-1,\n",
    "                    num_states=cliff_env.nS,\n",
    "                    num_actions=cliff_env.nA,\n",
    "                    epsilon=0.1)\n",
    "\n",
    "q_agent = QLearner(gamma=.9,\n",
    "                   lr=1e-1,\n",
    "                   num_states=cliff_env.nS,\n",
    "                   num_actions=cliff_env.nA,\n",
    "                   epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`env/toy_text/cliffwalking.py' 파일에서 수정\n",
    "```\n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, t = transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return (int(s), r, t, {\"prob\": p})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험진행에 사용할 코드\n",
    "\n",
    "두개의 에이전트를 주어진 환경과 상호작용하고 그 상호작용의 결과로 얻어진 데이터로 학습시키는 코드를 준비했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sarsa(agent, env):\n",
    "    \n",
    "    env.reset()\n",
    "    reward_sum = 0\n",
    "    \n",
    "    while True:\n",
    "        state = env.s\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_action = sarsa_agent.get_action(next_state)\n",
    "        reward_sum += reward\n",
    "        \n",
    "\n",
    "        agent.update_sample(state=state,\n",
    "                            action=action,\n",
    "                            reward=reward,\n",
    "                            next_state=next_state,\n",
    "                            next_action=next_action,\n",
    "                            done=done)\n",
    "    \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return reward_sum\n",
    "\n",
    "def run_qlearning(agent, env):\n",
    "    \n",
    "    env.reset()\n",
    "    reward_sum = 0\n",
    "    counter = 0\n",
    "    \n",
    "    while True:\n",
    "        state = env.s\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        agent.update_sample(state=state,\n",
    "                            action=action,\n",
    "                            reward=reward,\n",
    "                            next_state=next_state,\n",
    "                            done=done)    \n",
    "    \n",
    "        if done:\n",
    "            break\n",
    "                \n",
    "    return reward_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험을 진행해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/[Model-free control 03] SARSA vs Q-Learning.ipynb Cell 17\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_eps):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     sarsa_reward_sum \u001b[39m=\u001b[39m run_sarsa(sarsa_agent, cliff_env)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     qlearning_reward_sum \u001b[39m=\u001b[39m run_qlearning(q_agent, cliff_env)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     sarsa_rewards\u001b[39m.\u001b[39mappend(sarsa_reward_sum)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     qlearning_rewards\u001b[39m.\u001b[39mappend(qlearning_reward_sum)\n",
      "\u001b[1;32m/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/[Model-free control 03] SARSA vs Q-Learning.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39ms\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(state)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m next_state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m reward_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m agent\u001b[39m.\u001b[39mupdate_sample(state\u001b[39m=\u001b[39mstate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                     action\u001b[39m=\u001b[39maction,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                     reward\u001b[39m=\u001b[39mreward,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                     next_state\u001b[39m=\u001b[39mnext_state,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yoonju/RL_study/ReinforcementLearningAtoZ/PART2/%5BModel-free%20control%2003%5D%20SARSA%20vs%20Q-Learning.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                     done\u001b[39m=\u001b[39mdone)    \n",
      "File \u001b[0;32m~/anaconda3/envs/RL101/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py:153\u001b[0m, in \u001b[0;36mCliffWalkingEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "File \u001b[0;32m~/anaconda3/envs/RL101/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py:175\u001b[0m, in \u001b[0;36mCliffWalkingEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    174\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL101/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py:262\u001b[0m, in \u001b[0;36mCliffWalkingEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    260\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m    261\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m--> 262\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    263\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# rgb_array\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[1;32m    265\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    266\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_eps = 1500\n",
    "\n",
    "sarsa_rewards = []\n",
    "qlearning_rewards = []\n",
    "\n",
    "for i in range(num_eps):\n",
    "    sarsa_reward_sum = run_sarsa(sarsa_agent, cliff_env)\n",
    "    qlearning_reward_sum = run_qlearning(q_agent, cliff_env)\n",
    "    \n",
    "    sarsa_rewards.append(sarsa_reward_sum)\n",
    "    qlearning_rewards.append(qlearning_reward_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "ax.grid()\n",
    "ax.plot(sarsa_rewards, label='SARSA episode reward')\n",
    "ax.plot(qlearning_rewards, label='Q-Learning episode reward', alpha=0.5)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어? off-policy 인 Q-learning의 보상이 더 높을줄 알았는데...?\n",
    "\n",
    "off-policy인 Q-learning의 정책이 on-policy 인 SARSA의 보상보다 낮은 결과를 episode reward 를 얻었네요? 이게 무슨일일까요?\n",
    "\n",
    "> 정답은 0이 아닌 $\\epsilon$에 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(20, 10))\n",
    "visualize_policy(ax[0], sarsa_agent.q, cliff_env.shape[0], cliff_env.shape[1])\n",
    "_ = ax[0].set_title(\"SARSA policy\")\n",
    "\n",
    "visualize_policy(ax[1], q_agent.q, cliff_env.shape[0], cliff_env.shape[1])\n",
    "_ = ax[1].set_title(\"Q-Learning greedy policy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
