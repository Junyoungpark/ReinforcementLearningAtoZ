{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..') # add project root to the python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monte_carlo import ExactMCAgent, MCAgent, run_episode\n",
    "from envs.gridworld import GridworldEnv\n",
    "from utils.grid_visualization import visualize_value_function, visualize_policy\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GridWorld` 초기화하기\n",
    "\n",
    "가로로 `nx` 개, 세로로 `ny` 개의 칸을 가진 `GridworldEnv`를 만듭니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 4, 4\n",
    "env = GridworldEnv([ny, nx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-carlo '에이전트' 초기화하기\n",
    "\n",
    "`MCAgent` 는 learning rate를 사용하는 버전의 Monte-carlo 정책평가를 수행합니다. MC policy evaluation은 다음과 같은 수식으로 행동 가치함수를 추산합니다.\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (G(s,a) - Q(s,a))$$\n",
    "\n",
    "여기서, $G(s,a)$ 는 상태-행동 $(s,a)$의 리턴 추산치의 합. $\\alpha$ 학습률. 우리가 평가하려는 정책은 행동 가치함수 $Q(s,a)$ 에 대한 '$\\epsilon$-탐욕적 정책' 이라고 생각해보겠습니다. 이제 한번 파이썬 구현체를 살펴보도록 할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent = MCAgent(gamma=1.0,\n",
    "                   lr=1e-3,\n",
    "                   num_states=nx * ny,\n",
    "                   num_actions=4,\n",
    "                   epsilon=1.0) # epsilon=1.0? -> 모든 행동을 같은 확률로 하는 정책 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험을 도와줄 Helper 함수; `run_episode()`\n",
    "\n",
    "앞서 __Monte-carlo 정책평가__ 실습에서 살펴봤듯, MC 기법은 하나의 에피소드가 끝난 후, 행동 가치함수를 업데이트 할 수 있습니다.\n",
    "즉, 모든 업데이트는 episodic 하게 이루어지게 되겠죠? 이번 실습에서도 앞선 실습과 유사한 방법을 활용해서 $Q(s,a)$를 추산해보도록 할까요?\n",
    "\n",
    "```python\n",
    "def run_episode(env, agent, timeout=1000):\n",
    "    env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    i = 0\n",
    "    timeouted = False\n",
    "    while True:\n",
    "        state = env.s\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            i += 1\n",
    "            if i >= timeout:\n",
    "                timeouted = True\n",
    "                break\n",
    "\n",
    "    if not timeouted:\n",
    "        episode = (states, actions, rewards)\n",
    "        agent.update(episode)\n",
    "```\n",
    "\n",
    "> `timeout` 은 MC 에이전트가 학습 중에 정책이 잘못 학습되어 하나의 에피소드가 끝나지 않을 경우에, 강제로 에피소드를 종료하는 장치입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5000):\n",
    "    run_episode(env, mc_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MCAgent.get_action()` 과 `MCAgent._policy_q` 살펴보기\n",
    "\n",
    "수업에서 사용하는 `MCAgent`는 `MCAgent.q` $Q(s,a)$ 와 $\\pi(a|s)$를 생성하기 위해 사용하는 `MCAgent._policy_q`를 __독립적으로__ 가지고 있습니다. 일반적인 구현에서는 여태까지 알고 있는 가장 최신의 $Q(s,a)$ 를 정책을 생성하기 위해 사용합니다. 하지만, 저희 예제는 각각의 policy evaluation 과정과 policy improvment 과정을 명확히 분리해서 보여주기 위해 두개의 개념을 분리했습니다. 이에 따라서, `MCAgent.get_action`은 `MCAgent._policy_q`을 활용해서 $\\pi(a|s)$를 생성합니다.\n",
    "\n",
    "```python\n",
    "class ExactMCAgent:\n",
    "    \"\"\"\n",
    "    The exact Monte-Carlo agent.\n",
    "    This agents performs value update as follows:\n",
    "    V(s) <- s(s) / n(s)\n",
    "    Q(s,a) <- s(s,a) / n(s,a)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 gamma: float,\n",
    "                 num_states: int,\n",
    "                 num_actions: int,\n",
    "                 epsilon: float):\n",
    "        self.gamma = gamma\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self._eps = 1e-10  # for stable computation of V and Q. NOT the one for e-greedy!\n",
    "        ...\n",
    "        \n",
    "        # Initialize \"policy Q\"\n",
    "        # \"policy Q\" is the one used for policy generation.\n",
    "        self._policy_q = None\n",
    "        self.reset_policy()\n",
    "        ...\n",
    "    \n",
    "    def reset_policy(self):\n",
    "        self._policy_q = np.zeros(shape=(self.num_states, self.num_actions))\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        prob = np.random.uniform(0.0, 1.0, 1)\n",
    "        # e-greedy policy over Q\n",
    "        if prob <= self.epsilon:  # random\n",
    "            action = np.random.choice(range(self.num_actions))\n",
    "        else:  # greedy\n",
    "            action = self._policy_q[state, :].argmax()\n",
    "        return action\n",
    "    \n",
    "    def improve_policy(self):\n",
    "        self._policy_q = self.q.copy()\n",
    "        self.reset_values()\n",
    "        self.reset_statistics()\n",
    "        \n",
    "    def reset_statistics(self):\n",
    "        self.n_v = np.zeros(shape=self.num_states)\n",
    "        self.s_v = np.zeros(shape=self.num_states)\n",
    "\n",
    "        self.n_q = np.zeros(shape=(self.num_states, self.num_actions))\n",
    "        self.s_q = np.zeros(shape=(self.num_states, self.num_actions))\n",
    "\n",
    "    def reset_values(self):\n",
    "        self.v = np.zeros(shape=self.num_states)\n",
    "        self.q = np.zeros(shape=(self.num_states, self.num_actions))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent.improve_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent._policy_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추산된 상태가치함수 $V(s)$ 및 $Q(s,a)$ 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "visualize_value_function(ax[0], mc_agent.v, nx, ny)\n",
    "_ = ax[0].set_title(\"Value pi\")\n",
    "visualize_policy(ax[1], mc_agent.q, nx, ny)\n",
    "_ = ax[1].set_title(\"Greedy policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series green.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "def visualize_q(q):\n",
    "    df = pd.DataFrame(q, columns=['up', 'right', 'down', 'left']).T\n",
    "    df = df.style.apply(highlight_max)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = visualize_q(mc_agent.q)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with $\\epsilon$ !\n",
    "\n",
    "지금까지는 $\\epsilon$-탐욕적 정책에서 $\\epsilon=1$인 경우에 대해 정책을 평가했었습니다. 이제 한번 epsilon 을 조금씩 줄여가면서, 최적 정책 및 최적 상태 및 행동 가치함수를 찾아보도록 할까요? 아래의 함수를 활용해서 $\\epsilon$을 한번 조정해봅시다.\n",
    "\n",
    "```python\n",
    "def decaying_epsilon(self, factor):\n",
    "    self.epsilon *= factor\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decaying_epsilon_and_run(agent, env,\n",
    "                             decaying_factor:float,\n",
    "                             n_runs:int = 5000):\n",
    "\n",
    "    agent.decaying_epsilon(decaying_factor)\n",
    "    agent.reset_statistics()\n",
    "    \n",
    "    print(\"epsilon : {}\".format(agent.epsilon))\n",
    "\n",
    "    for _ in range(n_runs):\n",
    "        run_episode(env, agent)\n",
    "\n",
    "    agent.improve_policy()\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "    visualize_value_function(ax[0], agent.v, nx, ny)\n",
    "    _ = ax[0].set_title(\"Value pi\")\n",
    "    visualize_policy(ax[1], agent._policy_q, nx, ny)\n",
    "    _ = ax[1].set_title(\"Greedy policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decaying_epsilon_and_run(mc_agent, env, 0.9)\n",
    "decaying_epsilon_and_run(mc_agent, env, 0.9)\n",
    "decaying_epsilon_and_run(mc_agent, env, 0.1)\n",
    "decaying_epsilon_and_run(mc_agent, env, 0.1)\n",
    "decaying_epsilon_and_run(mc_agent, env, 0.1)\n",
    "decaying_epsilon_and_run(mc_agent, env, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$ decaying 이전에 이미 최적 정책함수를 찾았던거 기억하세요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent_non_decay = MCAgent(gamma=1.0,\n",
    "                             lr=1e-3,\n",
    "                             num_states=nx * ny,\n",
    "                             num_actions=4,\n",
    "                             epsilon=1.0) # epsilon=1.0? -> 모든 행동을 같은 확률로 하는 정책 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5000):\n",
    "    run_episode(env, mc_agent_non_decay)\n",
    "    \n",
    "mc_agent_non_decay.improve_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "visualize_value_function(ax[0], mc_agent_non_decay.v, nx, ny)\n",
    "_ = ax[0].set_title(\"Value pi\")\n",
    "visualize_policy(ax[1], mc_agent_non_decay.q, nx, ny)\n",
    "_ = ax[1].set_title(\"Greedy policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적 정책함수 $\\pi^{*}(a|s)$ 와 MC 기법을 활용해서 최적 가치함수 추정하기\n",
    "\n",
    "MC agent는 랜덤 정책으로부터 최적 정책을 만드는 $Q(s,a)$를 찾을 수 있었습니다. 그렇다면 그 $Q(s,a)$로 만들어진 \n",
    "greedy 정책을 가지고 Monte-carlo 정책평가를 수행하면 최적 정책함수 $V^{*}(s)$를 찾을 수 있을까요? 당연히 가능하겠지만, 간단한 실험을 통해서\n",
    "확인해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decaying_epsilon_and_run(mc_agent_non_decay, env, 0.0, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성급한 decaying\n",
    "\n",
    "수업에서도 다뤘듯, $\\epsilon$을 decaying 할 때는 적당한 schedule 을 사용하는 것이 필요합니다. \n",
    "만약 그렇게 못하면, `MCAgent`는 최적 정책을 찾지 못하게 되겠죠.\n",
    "\n",
    "한번 간단한 실험을 진행해볼까요? 극단적인 경우로, $\\epsilon$ 으로 `MCAgent`을 하나 만들어봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_mc_agent = MCAgent(gamma=1.0,\n",
    "                          lr=1e-3,\n",
    "                          num_states=nx * ny,\n",
    "                          num_actions=4,\n",
    "                          epsilon=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조금 오래 걸립니다. 실행 중간에 멈추지 마세요 :)\n",
    "# 많은 경우는 timeout이 걸릴때까지 episode를 진행하기 때문입니다.\n",
    "\n",
    "decaying_epsilon_and_run(greedy_mc_agent, env, 0.0, 5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
